{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4709b9f",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "Just predicting that there are never any cancellations at all, given that the percentage of cancelled flights is ~2.64%. This represents a naive baseline where we always predict \"no cancellation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44843d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2,400,000\n",
      "Test set size: 600,000\n",
      "\n",
      "Cancellation rate in test set: 0.0264 (2.64%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix, roc_auc_score, average_precision_score\n",
    "\n",
    "csv_path = \"../data/flights_sample_3m.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Similar to what was done in feature_engineering.ipynb\n",
    "\n",
    "# Feature Engineering: Add temporal features\n",
    "df[\"dep_hour\"] = df[\"CRS_DEP_TIME\"] // 100\n",
    "\n",
    "# Extract temporal features from FL_DATE\n",
    "df['FL_DATE'] = pd.to_datetime(df['FL_DATE'])\n",
    "df['month'] = df['FL_DATE'].dt.month\n",
    "df['day_of_week'] = df['FL_DATE'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "\n",
    "# Frequency Encoding (can be done before train/test split since it doesn't use target)\n",
    "# Count how often each airport appears\n",
    "df[\"origin_freq\"] = df[\"ORIGIN\"].map(df[\"ORIGIN\"].value_counts())\n",
    "df[\"dest_freq\"] = df[\"DEST\"].map(df[\"DEST\"].value_counts())\n",
    "\n",
    "# Get proportions\n",
    "df[\"origin_freq_proportion\"] = df[\"ORIGIN\"].map(df[\"ORIGIN\"].value_counts(normalize=True))\n",
    "df[\"dest_freq_proportion\"] = df[\"DEST\"].map(df[\"DEST\"].value_counts(normalize=True))\n",
    "\n",
    "# Drop leakage columns (actual times/delays that happen after scheduled departure)\n",
    "leakage_cols = [\n",
    "    'DEP_TIME', 'DEP_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'WHEELS_ON',\n",
    "    'TAXI_IN', 'ARR_TIME', 'ARR_DELAY', 'ELAPSED_TIME', 'AIR_TIME'\n",
    "]\n",
    "df = df.drop(columns=[col for col in leakage_cols if col in df.columns])\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[\"CANCELLED\"])  # Features\n",
    "y = df[\"CANCELLED\"]  # Target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train):,}\")\n",
    "print(f\"Test set size: {len(X_test):,}\")\n",
    "print(f\"\\nCancellation rate in test set: {y_test.mean():.4f} ({y_test.mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da5a1923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline predictions: (array([0.]), array([600000]))\n",
      "\n",
      "Actual test labels: (array([0., 1.]), array([584172,  15828]))\n"
     ]
    }
   ],
   "source": [
    "# Baseline: Predict all zeros (no cancellations)\n",
    "baseline_pred = np.zeros_like(y_test)\n",
    "\n",
    "print(f\"Baseline predictions: {np.unique(baseline_pred, return_counts=True)}\")\n",
    "print(f\"\\nActual test labels: {np.unique(y_test, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca836f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Performance:\n",
      "  Accuracy:  0.9736\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "  ROC-AUC:   0.5000\n",
      "  PR-AUC:    0.0264\n",
      "\n",
      "Confusion Matrix:\n",
      "[[584172      0]\n",
      " [ 15828      0]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline performance\n",
    "baseline_accuracy = accuracy_score(y_test, baseline_pred)\n",
    "baseline_precision = precision_score(y_test, baseline_pred, zero_division=0)\n",
    "baseline_recall = recall_score(y_test, baseline_pred, zero_division=0)\n",
    "\n",
    "# For ROC-AUC and PR-AUC, we need probability scores\n",
    "# Since baseline_pred is all zeros (binary predictions), we'll treat them as probability 0.0\n",
    "# Note: When all predictions are 0.0, ROC-AUC = 0.5 (random guessing) and PR-AUC = positive class rate\n",
    "baseline_proba = baseline_pred.astype(float)  # Convert to float (all 0.0)\n",
    "\n",
    "try:\n",
    "    baseline_roc_auc = roc_auc_score(y_test, baseline_proba)\n",
    "except ValueError as e:\n",
    "    baseline_roc_auc = 0.5  # When all predictions are 0.0, ROC-AUC equals 0.5 (diagonal ROC curve)\n",
    "    print(f\"Note: ROC-AUC calculation issue: {e}\")\n",
    "\n",
    "try:\n",
    "    baseline_pr_auc = average_precision_score(y_test, baseline_proba)\n",
    "except ValueError as e:\n",
    "    baseline_pr_auc = y_test.mean()  # When all predictions are 0.0, PR-AUC equals the positive class rate\n",
    "    print(f\"Note: PR-AUC calculation issue: {e}\")\n",
    "\n",
    "print(\"Baseline Model Performance:\")\n",
    "print(f\"  Accuracy:  {baseline_accuracy:.4f}\")\n",
    "print(f\"  Precision: {baseline_precision:.4f}\")\n",
    "print(f\"  Recall:    {baseline_recall:.4f}\")\n",
    "print(f\"  ROC-AUC:   {baseline_roc_auc:.4f}\")\n",
    "print(f\"  PR-AUC:    {baseline_pr_auc:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, baseline_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e01521",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- This baseline predicts **no cancellations** for all flights\n",
    "- It achieves ~97.36% accuracy (simply because most flights aren't cancelled)\n",
    "- However, it has **0% recall** (fails to catch any cancellations) and **0% precision**\n",
    "- **ROC-AUC = 0.5000**: This equals random guessing performance. When all predictions are 0.0, the ROC curve becomes a diagonal line (TPR = FPR), giving AUC = 0.5. This is the baseline for ROC-AUC (worse than random would be < 0.5)\n",
    "- **PR-AUC = 0.0264**: This equals the positive class rate (2.64% cancellation rate). When a model predicts all negatives, the PR-AUC equals the proportion of positive examples. This represents the baseline for PR-AUC\n",
    "- Any real model should beat this baseline by achieving ROC-AUC > 0.5 and PR-AUC > 0.0264\n",
    "- **Note**: For imbalanced datasets, PR-AUC is often more informative than ROC-AUC, as it focuses on the minority class performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "392b92aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping unencoded categorical columns: ['AIRLINE_DOT', 'AIRLINE_CODE', 'ORIGIN', 'ORIGIN_CITY', 'DEST', 'DEST_CITY', 'CANCELLATION_CODE']\n",
      "\n",
      "Processed training shape: (2400000, 20)\n",
      "Processed test shape: (600000, 20)\n",
      "\n",
      "Data types: {dtype('float64'): 11, dtype('int64'): 7, dtype('int32'): 2}\n",
      "\n",
      "Key features being used:\n",
      "  - DISTANCE\n",
      "  - dep_hour\n",
      "  - month\n",
      "  - day_of_week\n",
      "  - origin_freq\n",
      "  - dest_freq\n",
      "  - origin_freq_proportion\n",
      "  - dest_freq_proportion\n",
      "  - airline_target_encoded\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: Apply target encoding (after train/test split to avoid leakage)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Make copies for preprocessing\n",
    "X_train_processed = X_train.copy()\n",
    "X_test_processed = X_test.copy()\n",
    "\n",
    "# Target Encoding for AIRLINE (using only training data to avoid leakage)\n",
    "# Calculate mean cancellation rate per airline from TRAINING data only\n",
    "if 'AIRLINE' in X_train_processed.columns:\n",
    "    # Create a temporary dataframe with aligned indices\n",
    "    temp_df = pd.DataFrame({\n",
    "        'AIRLINE': X_train_processed['AIRLINE'],\n",
    "        'CANCELLED': y_train.values\n",
    "    }, index=X_train_processed.index)\n",
    "    \n",
    "    airline_target_means = temp_df.groupby('AIRLINE')['CANCELLED'].mean()\n",
    "    \n",
    "    # Apply to both train and test\n",
    "    X_train_processed['airline_target_encoded'] = X_train_processed['AIRLINE'].map(airline_target_means)\n",
    "    X_test_processed['airline_target_encoded'] = X_test_processed['AIRLINE'].map(airline_target_means)\n",
    "    \n",
    "    # Fill any unseen airlines in test with global training mean\n",
    "    global_mean = y_train.mean()\n",
    "    X_test_processed['airline_target_encoded'] = X_test_processed['airline_target_encoded'].fillna(global_mean)\n",
    "    \n",
    "    # Also fill any NaN in train (shouldn't happen, but just in case)\n",
    "    X_train_processed['airline_target_encoded'] = X_train_processed['airline_target_encoded'].fillna(global_mean)\n",
    "    \n",
    "    # Drop original AIRLINE column (we have the encoded version)\n",
    "    X_train_processed = X_train_processed.drop(columns=['AIRLINE'])\n",
    "    X_test_processed = X_test_processed.drop(columns=['AIRLINE'])\n",
    "\n",
    "# Select final feature set: use frequency-encoded and target-encoded features\n",
    "# Drop remaining categorical columns that aren't encoded yet\n",
    "categorical_cols = X_train_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "if categorical_cols:\n",
    "    print(f\"Dropping unencoded categorical columns: {categorical_cols}\")\n",
    "    X_train_processed = X_train_processed.drop(columns=categorical_cols)\n",
    "    X_test_processed = X_test_processed.drop(columns=categorical_cols)\n",
    "\n",
    "# Drop any columns that shouldn't be used (like CANCELLATION_CODE - only exists for cancelled flights)\n",
    "columns_to_drop = ['CANCELLATION_CODE'] if 'CANCELLATION_CODE' in X_train_processed.columns else []\n",
    "if columns_to_drop:\n",
    "    X_train_processed = X_train_processed.drop(columns=columns_to_drop)\n",
    "    X_test_processed = X_test_processed.drop(columns=columns_to_drop)\n",
    "\n",
    "# Fill any remaining NaN values with median (for numerical) or 0\n",
    "# Only fill numeric columns\n",
    "numeric_cols = X_train_processed.select_dtypes(include=[np.number]).columns\n",
    "X_train_processed[numeric_cols] = X_train_processed[numeric_cols].fillna(X_train_processed[numeric_cols].median())\n",
    "X_test_processed[numeric_cols] = X_test_processed[numeric_cols].fillna(X_train_processed[numeric_cols].median())\n",
    "\n",
    "# Ensure all columns are numeric (convert any remaining non-numeric to numeric)\n",
    "for col in X_train_processed.columns:\n",
    "    if X_train_processed[col].dtype == 'object':\n",
    "        # Try to convert to numeric\n",
    "        X_train_processed[col] = pd.to_numeric(X_train_processed[col], errors='coerce')\n",
    "        X_test_processed[col] = pd.to_numeric(X_test_processed[col], errors='coerce')\n",
    "\n",
    "# Final check: ensure no NaN or infinite values\n",
    "X_train_processed = X_train_processed.replace([np.inf, -np.inf], np.nan)\n",
    "X_test_processed = X_test_processed.replace([np.inf, -np.inf], np.nan)\n",
    "X_train_processed = X_train_processed.fillna(X_train_processed.median())\n",
    "X_test_processed = X_test_processed.fillna(X_train_processed.median())\n",
    "\n",
    "# Convert to numpy arrays for sklearn (or keep as DataFrames - sklearn accepts both)\n",
    "# But ensure all are numeric\n",
    "X_train_processed = X_train_processed.select_dtypes(include=[np.number])\n",
    "X_test_processed = X_test_processed.select_dtypes(include=[np.number])\n",
    "\n",
    "print(f\"\\nProcessed training shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed test shape: {X_test_processed.shape}\")\n",
    "print(f\"\\nData types: {X_train_processed.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"\\nKey features being used:\")\n",
    "feature_list = [col for col in X_train_processed.columns if any(x in col.lower() for x in ['freq', 'target', 'hour', 'month', 'day', 'distance'])]\n",
    "for feat in feature_list[:10]:  # Show first 10\n",
    "    print(f\"  - {feat}\")\n",
    "if len(feature_list) > 10:\n",
    "    print(f\"  ... and {len(X_train_processed.columns) - 10} more features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a9c34ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate models\n",
    "def evaluate_model(y_true, y_pred, y_proba, model_name):\n",
    "    \"\"\"Evaluate model performance and return metrics dictionary\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    roc_auc = roc_auc_score(y_true, y_proba)\n",
    "    pr_auc = average_precision_score(y_true, y_proba)\n",
    "    \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'PR-AUC': pr_auc\n",
    "    }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b98dda",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "**Fair Comparison Setup:**\n",
    "- All models use the **same decision threshold (0.5)** - this is the default for `.predict()`\n",
    "- All models handle class imbalance equivalently:\n",
    "  - Logistic Regression & Random Forest: `class_weight='balanced'`\n",
    "  - XGBoost & LightGBM: `scale_pos_weight` (equivalent approach)\n",
    "- All models use `random_state=42` for reproducibility\n",
    "- All models are evaluated on the same train/test split\n",
    "- Threshold optimization will be done later after selecting the best model\n",
    "\n",
    "**Key Metrics for Imbalanced Data:**\n",
    "- **PR-AUC**: Primary metric (focuses on minority class)\n",
    "- **ROC-AUC**: Secondary metric (overall discrimination ability)\n",
    "- **Recall**: How many cancellations we catch\n",
    "- **Precision**: How many predicted cancellations are real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269a59a2",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cbfec08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Performance:\n",
      "  Accuracy: 0.5309\n",
      "  Precision: 0.0424\n",
      "  Recall: 0.7782\n",
      "  ROC-AUC: 0.6896\n",
      "  PR-AUC: 0.0441\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Predictions\n",
    "lr_pred = lr_model.predict(X_test_processed)\n",
    "lr_proba = lr_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "lr_results = evaluate_model(y_test, lr_pred, lr_proba, 'Logistic Regression')\n",
    "print(\"\\nLogistic Regression Performance:\")\n",
    "for metric, value in lr_results.items():\n",
    "    if metric != 'Model':\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45513d9",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7df0b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Performance:\n",
      "  Accuracy: 0.7727\n",
      "  Precision: 0.0730\n",
      "  Recall: 0.6510\n",
      "  ROC-AUC: 0.8022\n",
      "  PR-AUC: 0.1068\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Using a subset for faster training - can adjust n_estimators and remove sampling for full training\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,  # Reduce for faster training; increase for better performance\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    max_depth=20,  # Limit depth for faster training\n",
    "    min_samples_split=100,\n",
    "    min_samples_leaf=50\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Predictions\n",
    "rf_pred = rf_model.predict(X_test_processed)\n",
    "rf_proba = rf_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "rf_results = evaluate_model(y_test, rf_pred, rf_proba, 'Random Forest')\n",
    "print(\"\\nRandom Forest Performance:\")\n",
    "for metric, value in rf_results.items():\n",
    "    if metric != 'Model':\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4556affb",
   "metadata": {},
   "source": [
    "## 3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab140f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Performance:\n",
      "  Accuracy: 0.6684\n",
      "  Precision: 0.0579\n",
      "  Recall: 0.7580\n",
      "  ROC-AUC: 0.7858\n",
      "  PR-AUC: 0.0866\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    # Calculate scale_pos_weight for class imbalance\n",
    "    # scale_pos_weight = number of negative samples / number of positive samples\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss',\n",
    "        tree_method='hist'\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    xgb_pred = xgb_model.predict(X_test_processed)\n",
    "    xgb_proba = xgb_model.predict_proba(X_test_processed)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    xgb_results = evaluate_model(y_test, xgb_pred, xgb_proba, 'XGBoost')\n",
    "    print(\"\\nXGBoost Performance:\")\n",
    "    for metric, value in xgb_results.items():\n",
    "        if metric != 'Model':\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"XGBoost not installed. Install with: pip install xgboost\")\n",
    "    xgb_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36c369d",
   "metadata": {},
   "source": [
    "## 4. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8a700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM Performance:\n",
      "  Accuracy: 0.6730\n",
      "  Precision: 0.0586\n",
      "  Recall: 0.7560\n",
      "  ROC-AUC: 0.7876\n",
      "  PR-AUC: 0.0891\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    # Calculate scale_pos_weight for class imbalance\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    lgb_model.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    lgb_pred = lgb_model.predict(X_test_processed)\n",
    "    lgb_proba = lgb_model.predict_proba(X_test_processed)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    lgb_results = evaluate_model(y_test, lgb_pred, lgb_proba, 'LightGBM')\n",
    "    print(\"\\nLightGBM Performance:\")\n",
    "    for metric, value in lgb_results.items():\n",
    "        if metric != 'Model':\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"LightGBM not installed. Install with: pip install lightgbm\")\n",
    "    lgb_results = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7442936f",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d59fbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL COMPARISON\n",
      "======================================================================\n",
      "                      Accuracy  Precision  Recall  ROC-AUC  PR-AUC\n",
      "Model                                                             \n",
      "Baseline (All Zeros)    0.9736     0.0000  0.0000   0.5000  0.0264\n",
      "Logistic Regression     0.5309     0.0424  0.7782   0.6896  0.0441\n",
      "Random Forest           0.7727     0.0730  0.6510   0.8022  0.1068\n",
      "XGBoost                 0.6684     0.0579  0.7580   0.7858  0.0866\n",
      "LightGBM                0.6730     0.0586  0.7560   0.7876  0.0891\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Best performing models by metric:\n",
      "  Accuracy    : Baseline (All Zeros) (0.9736)\n",
      "  Precision   : Random Forest        (0.0730)\n",
      "  Recall      : Logistic Regression  (0.7782)\n",
      "  ROC-AUC     : Random Forest        (0.8022)\n",
      "  PR-AUC      : Random Forest        (0.1068)\n"
     ]
    }
   ],
   "source": [
    "# Collect all results\n",
    "all_results = [lr_results, rf_results]\n",
    "\n",
    "if xgb_results:\n",
    "    all_results.append(xgb_results)\n",
    "if lgb_results:\n",
    "    all_results.append(lgb_results)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_results)\n",
    "comparison_df = comparison_df.set_index('Model')\n",
    "\n",
    "# Add baseline for comparison\n",
    "baseline_results = {\n",
    "    'Model': 'Baseline (All Zeros)',\n",
    "    'Accuracy': baseline_accuracy,\n",
    "    'Precision': baseline_precision,\n",
    "    'Recall': baseline_recall,\n",
    "    'ROC-AUC': baseline_roc_auc,\n",
    "    'PR-AUC': baseline_pr_auc\n",
    "}\n",
    "baseline_df = pd.DataFrame([baseline_results]).set_index('Model')\n",
    "\n",
    "# Combine baseline with model results\n",
    "comparison_df = pd.concat([baseline_df, comparison_df])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.round(4))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Show which model performs best on each metric\n",
    "print(\"\\nBest performing models by metric:\")\n",
    "metrics_to_compare = ['Accuracy', 'Precision', 'Recall', 'ROC-AUC', 'PR-AUC']\n",
    "for metric in metrics_to_compare:\n",
    "    if metric in ['ROC-AUC', 'PR-AUC', 'Accuracy', 'Precision', 'Recall']:\n",
    "        best_idx = comparison_df[metric].idxmax()\n",
    "        best_value = comparison_df.loc[best_idx, metric]\n",
    "        print(f\"  {metric:12s}: {best_idx:20s} ({best_value:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b7e2e1",
   "metadata": {},
   "source": [
    "## Model Selection Decision\n",
    "\n",
    "**Fair Comparison Summary:**\n",
    "- All models were compared using reasonable default hyperparameters\n",
    "- Same decision threshold (0.5), same train/test split, same features\n",
    "- This comparison is **fair** - no model was given special tuning\n",
    "\n",
    "**Results:**\n",
    "- **Random Forest** performs best on key metrics (PR-AUC, ROC-AUC, Precision)\n",
    "- XGBoost and LightGBM underperformed with current hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
